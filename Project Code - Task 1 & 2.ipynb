{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95333728-be04-4cb4-be17-d8a33e16b492",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gym==0.23.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36bc8fb-e10b-41f4-97dc-a7173fa3dd21",
   "metadata": {},
   "source": [
    "# BASIC TASKS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1d1861-a9a0-4bb3-b3a6-756f8649c027",
   "metadata": {},
   "source": [
    "## Building Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d74b47-ba59-4389-8019-e4c6e255b437",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcd4dfc-e8fd-4910-adc1-893698b12ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Maze Environment\n",
    "class MazeEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}   # Setting rendering to 'human' for display of the agent's behaviour\n",
    "\n",
    "    def __init__(self, grid_size=5):\n",
    "        super(MazeEnv, self).__init__()\n",
    "        self.grid_size = grid_size   # Storing grid size = 5\n",
    "        self.action_space = spaces.Discrete(4)   # Defining action space - agent can move in 4 directions: up, down, left, right\n",
    "        self.observation_space = spaces.Discrete(grid_size * grid_size) # 5x5 2D grid\n",
    "        self.has_key = False   # Tracking key collection\n",
    "        self._create_maze()   # Building maze\n",
    "\n",
    "    def _create_maze(self):\n",
    "        self.grid = np.zeros((self.grid_size, self.grid_size), dtype=np.uint8)   # Building empty grid\n",
    "        self.agent_pos = [0,0]   # Agent starting position top left corner\n",
    "        self.goal_pos = [self.grid_size - 1, self.grid_size - 1]   # Goal position at bottom right (treasure)\n",
    "        self.grid[tuple(self.goal_pos)] = 4   # Goal - Treasure\n",
    "        self.grid[(3,2)] = 3   # Trap 2\n",
    "        self.grid[(1,3)] = 2   # Trap 1\n",
    "        self.grid[(2,4)] = 1   # Key\n",
    "\n",
    "    # Resetting environment\n",
    "    def reset(self):\n",
    "        self._create_maze()\n",
    "        self.has_key = False   # Resetting key collection status\n",
    "        self.agent_pos = [0,0]   # Putting agent back to starting position\n",
    "        return self._get_state()   # Returning initial state\n",
    "\n",
    "    # Defining current state\n",
    "    def _get_state(self):\n",
    "        y,x = self.agent_pos\n",
    "        return y * self.grid_size + x\n",
    "\n",
    "    def step(self, action):\n",
    "        y, x = self.agent_pos\n",
    "        # Moving agent\n",
    "        if action == 0 and y>0:   # Moving Up\n",
    "            y -= 1\n",
    "        elif action == 1 and x<self.grid_size - 1:   # Moving Right\n",
    "            x += 1\n",
    "        elif action == 2 and y<self.grid_size - 1:   # Moving Down\n",
    "            y += 1\n",
    "        elif action == 3 and x>0:   # Moving Left\n",
    "            x -= 1\n",
    "\n",
    "        self.agent_pos = [y,x]\n",
    "        reward = -1.0   # Penalty for every step taken to incentivise quicker paths\n",
    "        done = False\n",
    "\n",
    "        tile = self.grid[y,x]\n",
    "        if tile == 1:\n",
    "            reward = 15\n",
    "            self.has_key = True   # Agent collected key\n",
    "            self.grid[y,x] = 0   # Removing key after collection\n",
    "        elif tile == 2:\n",
    "            reward = -10   # Agent encountered a trap\n",
    "        elif tile == 3:\n",
    "            reward = -10   # Agent encountered a trap\n",
    "        elif tile == 4:\n",
    "            if self.has_key:\n",
    "                reward = 30   # Agent reached treasure with key\n",
    "                done = True   # Terminating episode\n",
    "            else:\n",
    "                reward = -3   # Giving negative reward if agent tries finishing episode without key\n",
    "                done = False   # Episode continues as agent did not collect key\n",
    "\n",
    "        # Returning updated observation, reward, done flag\n",
    "        return self._get_state(), reward, done, {}\n",
    "\n",
    "    # Code for the following render function was taken from the video in [1].\n",
    "    # Defining render function for display\n",
    "    def render(self, mode='human'):\n",
    "        print(\"\\nCurrent Maze State:\")\n",
    "        for r in range(self.grid_size):\n",
    "            for c in range(self.grid_size):\n",
    "                pos = [r, c]\n",
    "                if pos == self.agent_pos:\n",
    "                    print('A', end=' ')   # Agent\n",
    "                elif pos == self.goal_pos:\n",
    "                    print('$', end=' ')   # Treasure\n",
    "                elif pos == [3, 2]:\n",
    "                    print('X', end=' ')   # Trap\n",
    "                elif pos == [1, 3]:\n",
    "                    print('X', end=' ')   # Trap\n",
    "                elif pos == [2, 4]:\n",
    "                    print('K', end=' ')   # Key\n",
    "                else:\n",
    "                    print('.', end=' ')   # Empty \n",
    "            print()\n",
    "        print()\n",
    "\n",
    "    def close(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce823b59-9a17-4f6e-8cdc-f843465cde5d",
   "metadata": {},
   "source": [
    "## Setting up Q-Learning [2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9361b83-fb3d-4c44-8fdd-1a5efe588bfc",
   "metadata": {},
   "source": [
    "The following code was mainly taken from lab tutorial 4 part 2 [2]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5e2ffe-1865-47c3-abeb-b3a399e67323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code was mainly taken from lab tutorial 4 part 2 [2].\n",
    "\n",
    "# Defining function to initialise Qtable\n",
    "def initialize_q_table(state_space, action_space):\n",
    "    return np.zeros((state_space, action_space))\n",
    "\n",
    "# Defining epsilon-greedy policy function\n",
    "def epsilon_greedy_policy(Qtable, state, epsilon):\n",
    "    if np.random.uniform(0,1) < epsilon:\n",
    "        return np.random.choice(Qtable.shape[1])\n",
    "    else:\n",
    "        return np.argmax(Qtable[state])\n",
    "\n",
    "# Training Step Function\n",
    "def train(n_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable, alpha, gamma):\n",
    "    # Define empty rewards and steps lists\n",
    "    episode_rewards = []\n",
    "    episode_steps = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        # Reduce epsilon - less and less exploration is needed\n",
    "        epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate * episode)\n",
    "        # Reset the environment\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "\n",
    "        # Priniting epsiode number\n",
    "        #print(f\"\\n--- Episode {episode + 1} ---\")\n",
    "        \n",
    "        for _ in range(max_steps):\n",
    "            action = epsilon_greedy_policy(Qtable, state, epsilon)\n",
    "            new_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            # Render current state of the environment \n",
    "            #env.render()\n",
    "            #time.sleep(0.1)\n",
    "            \n",
    "            # Updating Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "            Qtable[state, action] += alpha * (reward + gamma * np.max(Qtable[new_state]) - Qtable[state, action])\n",
    "\n",
    "            # The state is the new state\n",
    "            state = new_state\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            \n",
    "            # If done, finish the episode\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # Printing reward after every episode\n",
    "        print(f\"Episode {episode + 1}: Reward = {total_reward}\")\n",
    "        \n",
    "        episode_rewards.append(total_reward)\n",
    "        episode_steps.append(steps)\n",
    "            \n",
    "    return Qtable, episode_rewards, episode_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca9bcd4-3efa-4d70-9087-2c5b5b77eb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the environment as the above constructed maze\n",
    "env = MazeEnv(grid_size=5)\n",
    "# Calling state and action spaces\n",
    "state_space = env.observation_space.n\n",
    "action_space = env.action_space.n\n",
    "\n",
    "# Initilising Qtable\n",
    "Qtable = initialize_q_table(state_space, action_space)\n",
    "\n",
    "# Hyperparameters\n",
    "n_training_episodes = 1500   # Total training episodes\n",
    "max_steps = 100   # Maximum number of steps per episode\n",
    "alpha = 0.7   # Learning rate\n",
    "gamma = 0.95   # Discounting rate\n",
    "max_epsilon = 1.0   # Exploration probability at the start = 1.0 for maximum exploration\n",
    "min_epsilon = 0.05   # Minimum possible exploration probability\n",
    "decay_rate = 0.0005   # Exponential decay rate for exploration probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a61afc2-a1da-4c57-86bd-3c65c88c6fc6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setting random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Train\n",
    "Qtable, episode_rewards, episode_steps = train(\n",
    "    n_training_episodes,\n",
    "    min_epsilon, max_epsilon,\n",
    "    decay_rate,\n",
    "    env,\n",
    "    max_steps,\n",
    "    Qtable,\n",
    "    alpha,\n",
    "    gamma\n",
    ")\n",
    "\n",
    "# Printing the q-values for every state\n",
    "print(\"Final Q-table (state x action):\")\n",
    "for state in range(Qtable.shape[0]):\n",
    "    print(f\"State {state}: {Qtable[state]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fda6fc-efaa-479f-95e9-79a715fe3f95",
   "metadata": {},
   "source": [
    "## Model Evaluation [2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e325dd7d-626b-4d8c-bf0c-10c0815b69aa",
   "metadata": {},
   "source": [
    "The following code was mainly taken from lab tutorial 4 part 2 [2]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eba4e9f-5c7c-4cb3-b745-abcc036f8720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code was mainly taken from lab tutorial 4 part 2 [2].\n",
    "\n",
    "# Evaluates the agent for 'n_eval_episodes' episodes and returns average reward and std of reward\n",
    "def evaluate_agent(env, Qtable, max_steps, n_eval_episodes=150):\n",
    "    # env: Evaluation environment\n",
    "    # Qtable: The Q-table\n",
    "    # n_eval_episodes: Number of episode to evaluate the agent\n",
    "    rewards = []\n",
    "    for _ in range(n_eval_episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        \n",
    "        for _ in range(max_steps):\n",
    "            action = np.argmax(Qtable[state])\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        rewards.append(total_reward)\n",
    "    return np.mean(rewards), np.std(rewards)\n",
    "\n",
    "# Evaluate\n",
    "mean_reward, std_reward = evaluate_agent(env, Qtable, max_steps)\n",
    "print(f\"Evaluation results -> Mean reward = {mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921961ab-cb74-40da-9791-5071b142777a",
   "metadata": {},
   "source": [
    "## Plotting Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96249b8-f1c0-48d8-8150-4db66928150d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining moving average function to plot average reward per episode learning curve\n",
    "def moving_average(data, window_size=50):   # Setting window_size=50 --> Averaging over 50 episodes\n",
    "    return np.convolve(data, np.ones(window_size)/window_size, mode='valid')\n",
    "avg_rewards = moving_average(episode_rewards, window_size=50)\n",
    "avg_steps = moving_average(episode_steps, window_size=50)\n",
    "\n",
    "fig, axs = plt.subplots(4, 1, figsize=(10,8), sharex=True)\n",
    "\n",
    "axs[0].plot(episode_rewards)\n",
    "axs[0].set_title('Total Reward vs Episode')\n",
    "axs[0].set_ylabel('Total Reward')\n",
    "axs[0].grid(True)\n",
    "\n",
    "axs[1].plot(episode_steps)\n",
    "axs[1].set_title('Steps per Episode')\n",
    "axs[1].set_ylabel('Steps')\n",
    "axs[1].grid(True)\n",
    "\n",
    "axs[2].plot(range(len(avg_rewards)), avg_rewards, label='Average Reward')\n",
    "axs[2].set_title('Average Reward per Episode')\n",
    "axs[2].set_ylabel('Average Reward')\n",
    "axs[2].grid(True)\n",
    "\n",
    "axs[3].plot(range(len(avg_steps)), avg_steps, label='Average Steps')\n",
    "axs[3].set_title('Average Steps per Episode')\n",
    "axs[3].set_xlabel('Episode')\n",
    "axs[3].set_ylabel('Average Steps')\n",
    "axs[3].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a047b92d-7c67-43c5-92d6-fc9bcbaf5496",
   "metadata": {},
   "source": [
    "## Experiment with Different Parameter Values & Policies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdd6f8b-a9df-48a4-b061-9626feacc41e",
   "metadata": {},
   "source": [
    "## Varying Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752d7203-6ec8-4141-830a-5bacc9c57867",
   "metadata": {},
   "source": [
    "Leaving gamma and decay rate fixed while changing alpha parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e09bc1b-b5a9-4cc2-bf18-f46819be0678",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "n_training_episodes = 1500   # Total training episodes\n",
    "max_steps = 100   # Maximum number of steps per episode\n",
    "gamma = 0.95   # Discounting rate\n",
    "max_epsilon = 1.0   # Exploration probability at the start = 1.0 for maximum exploration\n",
    "min_epsilon = 0.05   # Minimum possible exploration probability\n",
    "decay_rate = 0.0005   # Exponential decay rate for exploration probability\n",
    "\n",
    "alphas = [0.5, 0.9]   # Learning rate\n",
    "colors = ['#4D4D4D', '#DD8452']\n",
    "labels = ['alpha=0.5', 'alpha=0.9']\n",
    "\n",
    "all_rewards = []\n",
    "all_steps = []\n",
    "all_avg_rewards = []\n",
    "all_avg_steps = []\n",
    "\n",
    "# Setting random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "for idx, alpha in enumerate(alphas):\n",
    "    # Initialising Qtable before training\n",
    "    Qtable = initialize_q_table(state_space, action_space)\n",
    "    Qtable, episode_rewards, episode_steps = train(\n",
    "        n_training_episodes,\n",
    "        min_epsilon, max_epsilon,\n",
    "        decay_rate,\n",
    "        env,\n",
    "        max_steps,\n",
    "        Qtable,\n",
    "        alpha,\n",
    "        gamma\n",
    "    )\n",
    "\n",
    "    # Evaluate agent\n",
    "    mean_reward, std_reward = evaluate_agent(env, Qtable, max_steps)\n",
    "    print(f\"Alpha={alpha} -> Mean reward = {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "    \n",
    "    all_rewards.append(episode_rewards)\n",
    "    all_steps.append(episode_steps)\n",
    "    \n",
    "    # Calculate smoothed average rewards\n",
    "    def moving_average(data, window_size=50):\n",
    "        return np.convolve(data, np.ones(window_size)/window_size, mode='valid')\n",
    "\n",
    "    avg_rewards = moving_average(episode_rewards, window_size=50)\n",
    "    avg_steps = moving_average(episode_steps, window_size=50)\n",
    "    all_avg_rewards.append(avg_rewards)\n",
    "    all_avg_steps.append(avg_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed234515-3493-4485-8fd3-d7afa22e41ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting performance\n",
    "fig, axs = plt.subplots(4, 1, figsize=(10,8), sharex=True)\n",
    "\n",
    "for idx, rewards in enumerate(all_rewards):\n",
    "    axs[0].plot(rewards, label=labels[idx], color=colors[idx])\n",
    "axs[0].set_title('Total Reward per Episode')\n",
    "axs[0].set_ylabel('Total Reward')\n",
    "axs[0].grid(True)\n",
    "axs[0].legend(loc='upper left')\n",
    "\n",
    "for idx, steps in enumerate(all_steps):\n",
    "    axs[1].plot(steps, label=labels[idx], color=colors[idx])\n",
    "axs[1].set_title('Steps per Episode')\n",
    "axs[1].set_ylabel('Steps')\n",
    "axs[1].grid(True)\n",
    "axs[1].legend(loc='upper left')\n",
    "\n",
    "for idx, avg_rewards in enumerate(all_avg_rewards):\n",
    "    axs[2].plot(range(len(avg_rewards)), avg_rewards, label=labels[idx], color=colors[idx])\n",
    "axs[2].set_title('Average Reward per Episode')\n",
    "axs[2].set_ylabel('Average Reward')\n",
    "axs[2].grid(True)\n",
    "axs[2].legend(loc='upper left')\n",
    "\n",
    "for idx, avg_steps in enumerate(all_avg_steps):\n",
    "    axs[3].plot(range(len(avg_steps)), avg_steps, label=labels[idx], color=colors[idx])\n",
    "axs[3].set_title('Average Steps per Episode')\n",
    "axs[3].set_xlabel('Episode')\n",
    "axs[3].set_ylabel('Average Steps')\n",
    "axs[3].grid(True)\n",
    "axs[3].legend(loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b8e6e1-ea89-476f-be63-ba4996d0c404",
   "metadata": {},
   "source": [
    "Leaving alpha and decay rate fixed while changing gamma parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053ee235-67c5-4a7d-a876-43a64da79e07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "n_training_episodes = 1500   # Total training episodes\n",
    "max_steps = 100   # Maximum number of steps per episode\n",
    "alpha = 0.7   # Learning rate\n",
    "max_epsilon = 1.0   # Exploration probability at the start = 1.0 for maximum exploration\n",
    "min_epsilon = 0.05   # Minimum possible exploration probability\n",
    "decay_rate = 0.0005   # Exponential decay rate for exploration probability\n",
    "\n",
    "gammas = [0.9, 0.99]   # Discounting rate\n",
    "colors = ['#4D4D4D', '#DD8452']\n",
    "labels = ['gamma=0.9', 'gamma=0.99']\n",
    "\n",
    "all_rewards = []\n",
    "all_steps = []\n",
    "all_avg_rewards = []\n",
    "all_avg_steps = []\n",
    "\n",
    "# Setting random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "for idx, gamma in enumerate(gammas):\n",
    "    # Initialising Qtable before training\n",
    "    Qtable = initialize_q_table(state_space, action_space)\n",
    "    Qtable, episode_rewards, episode_steps = train(\n",
    "        n_training_episodes,\n",
    "        min_epsilon, max_epsilon,\n",
    "        decay_rate,\n",
    "        env,\n",
    "        max_steps,\n",
    "        Qtable,\n",
    "        alpha,\n",
    "        gamma\n",
    "    )\n",
    "\n",
    "    # Evaluate agent\n",
    "    mean_reward, std_reward = evaluate_agent(env, Qtable, max_steps)\n",
    "    print(f\"Gamma={gamma} -> Mean reward = {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "    \n",
    "    all_rewards.append(episode_rewards)\n",
    "    all_steps.append(episode_steps)\n",
    "    \n",
    "    # Calculate smoothed average rewards\n",
    "    def moving_average(data, window_size=50):\n",
    "        return np.convolve(data, np.ones(window_size)/window_size, mode='valid')\n",
    "\n",
    "    avg_rewards = moving_average(episode_rewards, window_size=50)\n",
    "    avg_steps = moving_average(episode_steps, window_size=50)\n",
    "    all_avg_rewards.append(avg_rewards)\n",
    "    all_avg_steps.append(avg_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b754c11-c99c-4f35-a672-5a98f9ff66a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting performance\n",
    "fig, axs = plt.subplots(4, 1, figsize=(10,8), sharex=True)\n",
    "\n",
    "for idx, rewards in enumerate(all_rewards):\n",
    "    axs[0].plot(rewards, label=labels[idx], color=colors[idx])\n",
    "axs[0].set_title('Total Reward per Episode')\n",
    "axs[0].set_ylabel('Total Reward')\n",
    "axs[0].grid(True)\n",
    "axs[0].legend(loc='upper left')\n",
    "\n",
    "for idx, steps in enumerate(all_steps):\n",
    "    axs[1].plot(steps, label=labels[idx], color=colors[idx])\n",
    "axs[1].set_title('Steps per Episode')\n",
    "axs[1].set_ylabel('Steps')\n",
    "axs[1].grid(True)\n",
    "axs[1].legend(loc='upper left')\n",
    "\n",
    "for idx, avg_rewards in enumerate(all_avg_rewards):\n",
    "    axs[2].plot(range(len(avg_rewards)), avg_rewards, label=labels[idx], color=colors[idx])\n",
    "axs[2].set_title('Average Reward per Episode')\n",
    "axs[2].set_ylabel('Average Reward')\n",
    "axs[2].grid(True)\n",
    "axs[2].legend(loc='upper left')\n",
    "\n",
    "for idx, avg_steps in enumerate(all_avg_steps):\n",
    "    axs[3].plot(range(len(avg_steps)), avg_steps, label=labels[idx], color=colors[idx])\n",
    "axs[3].set_title('Average Steps per Episode')\n",
    "axs[3].set_xlabel('Episode')\n",
    "axs[3].set_ylabel('Average Steps')\n",
    "axs[3].grid(True)\n",
    "axs[3].legend(loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed76c4a-b6c0-4ddd-84b9-e0b4c1e23379",
   "metadata": {},
   "source": [
    "Leaving alpha and gamma fixed while changing decay rate parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd222e36-7759-4ba1-9154-ab36cfdbccae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "n_training_episodes = 1500   # Total training episodes\n",
    "max_steps = 100   # Maximum number of steps per episode\n",
    "alpha = 0.7   # Learning rate\n",
    "max_epsilon = 1.0   # Exploration probability at the start = 1.0 for maximum exploration\n",
    "min_epsilon = 0.05   # Minimum possible exploration probability\n",
    "gamma = 0.95   # Discounting rate\n",
    "\n",
    "decay_rates = [0.001, 0.005]   # Exponential decay rate for exploration probability\n",
    "colors = ['#4D4D4D', '#DD8452']\n",
    "labels = ['decay_rate=0.001', 'decay_rate=0.005']\n",
    "\n",
    "all_rewards = []\n",
    "all_steps = []\n",
    "all_avg_rewards = []\n",
    "all_avg_steps = []\n",
    "\n",
    "# Setting random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "for idx, decay_rate in enumerate(decay_rates):\n",
    "    # Initialising Qtable before training\n",
    "    Qtable = initialize_q_table(state_space, action_space)\n",
    "    Qtable, episode_rewards, episode_steps = train(\n",
    "        n_training_episodes,\n",
    "        min_epsilon, max_epsilon,\n",
    "        decay_rate,\n",
    "        env,\n",
    "        max_steps,\n",
    "        Qtable,\n",
    "        alpha,\n",
    "        gamma\n",
    "    )\n",
    "\n",
    "    # Evaluate agent\n",
    "    mean_reward, std_reward = evaluate_agent(env, Qtable, max_steps)\n",
    "    print(f\"Decay rate={decay_rate} -> Mean reward = {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "    \n",
    "    all_rewards.append(episode_rewards)\n",
    "    all_steps.append(episode_steps)\n",
    "    \n",
    "    # Calculate smoothed average rewards\n",
    "    def moving_average(data, window_size=10):\n",
    "        return np.convolve(data, np.ones(window_size)/window_size, mode='valid')\n",
    "\n",
    "    avg_rewards = moving_average(episode_rewards, window_size=50)\n",
    "    avg_steps = moving_average(episode_steps, window_size=50)\n",
    "    all_avg_rewards.append(avg_rewards)\n",
    "    all_avg_steps.append(avg_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e75dc62-75a6-4540-bf77-4bfdb88efef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting performance\n",
    "fig, axs = plt.subplots(4, 1, figsize=(10,8), sharex=True)\n",
    "\n",
    "for idx, rewards in enumerate(all_rewards):\n",
    "    axs[0].plot(rewards, label=labels[idx], color=colors[idx])\n",
    "axs[0].set_title('Total Reward per Episode')\n",
    "axs[0].set_ylabel('Total Reward')\n",
    "axs[0].grid(True)\n",
    "axs[0].legend(loc='upper left')\n",
    "\n",
    "for idx, steps in enumerate(all_steps):\n",
    "    axs[1].plot(steps, label=labels[idx], color=colors[idx])\n",
    "axs[1].set_title('Steps per Episode')\n",
    "axs[1].set_ylabel('Steps')\n",
    "axs[1].grid(True)\n",
    "axs[1].legend(loc='upper left')\n",
    "\n",
    "for idx, avg_rewards in enumerate(all_avg_rewards):\n",
    "    axs[2].plot(range(len(avg_rewards)), avg_rewards, label=labels[idx], color=colors[idx])\n",
    "axs[2].set_title('Average Reward per Episode')\n",
    "axs[2].set_ylabel('Average Reward')\n",
    "axs[2].grid(True)\n",
    "axs[2].legend(loc='upper left')\n",
    "\n",
    "for idx, avg_steps in enumerate(all_avg_steps):\n",
    "    axs[3].plot(range(len(avg_steps)), avg_steps, label=labels[idx], color=colors[idx])\n",
    "axs[3].set_title('Average Steps per Episode')\n",
    "axs[3].set_xlabel('Episode')\n",
    "axs[3].set_ylabel('Average Steps')\n",
    "axs[3].grid(True)\n",
    "axs[3].legend(loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4677e0-bfd2-403b-a45d-bc60f482e97e",
   "metadata": {},
   "source": [
    "*Best parameter combination - which combination is the fastest to reach an average positive reward:*\n",
    "\n",
    "ALPHA=0.7\n",
    "\n",
    "GAMMA=0.95\n",
    "\n",
    "DECAY_RATE=0.005"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6401512b-9e52-4db2-8261-8cbcc0526e08",
   "metadata": {},
   "source": [
    "## Varying Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496698ca-d50c-46d6-a0bd-3bb6e7490b0d",
   "metadata": {},
   "source": [
    "Experimenting with softmax policy instead of epsilon-greedy policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39606428-6a96-4556-9d62-799f8a8f6a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining softmax policy\n",
    "def softmax_policy(Qtable, state, temperature=1.0):\n",
    "    q_values = Qtable[state]\n",
    "    exp_q = np.exp( q_values / temperature )\n",
    "    probs = exp_q / np.sum(exp_q)\n",
    "    return np.random.choice(len(q_values), p=probs)\n",
    "\n",
    "# Training Step\n",
    "def softmax_train(n_episodes, temperature, env, max_steps, Qtable, alpha, gamma):\n",
    "    episode_rewards = []\n",
    "    episode_steps = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        # Reset the environment\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "\n",
    "        #print(f\"\\n--- Episode {episode + 1} ---\")\n",
    "        \n",
    "        for _ in range(max_steps):\n",
    "            action = softmax_policy(Qtable, state, temperature)\n",
    "            new_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            # Render current state of the environment \n",
    "            #env.render()\n",
    "            #time.sleep(0.1)\n",
    "            \n",
    "            # Updating Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "            Qtable[state, action] += alpha * (reward + gamma * np.max(Qtable[new_state]) - Qtable[state, action])\n",
    "\n",
    "            # Our state is the new state\n",
    "            state = new_state\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            \n",
    "            # If done, finish the episode\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # Printing reward after every episode\n",
    "        print(f\"Episode {episode + 1}: Reward = {total_reward}\")\n",
    "\n",
    "        episode_rewards.append(total_reward)\n",
    "        episode_steps.append(steps)\n",
    "            \n",
    "    return Qtable, episode_rewards, episode_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c949d9c-5fa8-4ccd-9149-aa8597949f0a",
   "metadata": {},
   "source": [
    "## Evaluating and Plotting Performance with Different Parameter Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae66e203-dae4-41f1-bb8c-a2efee62ff7e",
   "metadata": {},
   "source": [
    "Leaving alpha and gamma fixed while changing temperature parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0876ba-0f63-4e2b-8517-bbc89be24030",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "n_training_episodes = 1500   # Total training episodes\n",
    "max_steps = 100   # Maximum number of steps per episode\n",
    "alpha = 0.5   # Learning rate\n",
    "gamma = 0.95   # Discounting rate\n",
    "\n",
    "temperatures = [2.5, 5.0, 7.5]   # Temperature\n",
    "colors = ['#55A868', '#C44E52', '#8172B2']\n",
    "labels = ['T=2.5', 'T=5.0', 'T=7.5']\n",
    "\n",
    "all_rewards = []\n",
    "all_steps = []\n",
    "all_avg_rewards = []\n",
    "all_avg_steps = []\n",
    "\n",
    "np.random.seed(42) # Setting random seed for reproducibility\n",
    "\n",
    "for idx, temp in enumerate(temperatures):\n",
    "    # Initialising Qtable before training\n",
    "    Qtable_softmax = initialize_q_table(state_space, action_space)\n",
    "    # Training with softmax\n",
    "    Qtable_softmax, softmax_rewards, softmax_steps = softmax_train(\n",
    "        n_training_episodes,\n",
    "        temp,\n",
    "        env,\n",
    "        max_steps,\n",
    "        Qtable_softmax,\n",
    "        alpha,\n",
    "        gamma\n",
    "    )\n",
    "\n",
    "    # Evaluation\n",
    "    mean_reward_softmax, std_reward_softmax = evaluate_agent(env, Qtable_softmax, max_steps)\n",
    "    print(f\"Temperature={temp} -> Mean reward = {mean_reward_softmax:.2f} +/- {std_reward_softmax:.2f}\")\n",
    "\n",
    "    all_rewards.append(softmax_rewards)\n",
    "    all_steps.append(softmax_steps)\n",
    "    \n",
    "    # Calculate smoothed average rewards\n",
    "    def moving_average(data, window_size=10):\n",
    "        return np.convolve(data, np.ones(window_size)/window_size, mode='valid')\n",
    "\n",
    "    avg_rewards = moving_average(softmax_rewards, window_size=50)\n",
    "    avg_steps = moving_average(softmax_steps, window_size=50)\n",
    "    all_avg_rewards.append(avg_rewards)\n",
    "    all_avg_steps.append(avg_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f965c6-159b-4a41-b1ef-d3c79887fd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting performance\n",
    "fig, axs = plt.subplots(4, 1, figsize=(10,8), sharex=True)\n",
    "\n",
    "for idx, rewards in enumerate(all_rewards):\n",
    "    axs[0].plot(rewards, label=labels[idx], color=colors[idx])\n",
    "axs[0].set_title('Total Reward per Episode')\n",
    "axs[0].set_ylabel('Total Reward')\n",
    "axs[0].grid(True)\n",
    "axs[0].legend(loc='upper left')\n",
    "\n",
    "for idx, steps in enumerate(all_steps):\n",
    "    axs[1].plot(steps, label=labels[idx], color=colors[idx])\n",
    "axs[1].set_title('Steps per Episode')\n",
    "axs[1].set_ylabel('Steps')\n",
    "axs[1].grid(True)\n",
    "axs[1].legend(loc='upper left')\n",
    "\n",
    "for idx, avg_rewards in enumerate(all_avg_rewards):\n",
    "    axs[2].plot(range(len(avg_rewards)), avg_rewards, label=labels[idx], color=colors[idx])\n",
    "axs[2].set_title('Average Reward per Episode')\n",
    "axs[2].set_ylabel('Average Reward')\n",
    "axs[2].grid(True)\n",
    "axs[2].legend(loc='upper left')\n",
    "\n",
    "for idx, avg_steps in enumerate(all_avg_steps):\n",
    "    axs[3].plot(range(len(avg_steps)), avg_steps, label=labels[idx], color=colors[idx])\n",
    "axs[3].set_title('Average Steps per Episode')\n",
    "axs[3].set_xlabel('Episode')\n",
    "axs[3].set_ylabel('Average Steps')\n",
    "axs[3].grid(True)\n",
    "axs[3].legend(loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7038db0a-b943-46ef-b865-a9941fbe03dd",
   "metadata": {},
   "source": [
    "Leaving temperature and gamma fixed while changing alpha parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28025d9-dc65-4611-919e-4645e8307fbf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "n_training_episodes = 1500   # Total training episodes\n",
    "max_steps = 100   # Maximum number of steps per episode\n",
    "temperature = 2.5   # Temperature\n",
    "gamma = 0.95   # Discounting rate\n",
    "\n",
    "alphas = [0.2, 0.5, 0.8]   # Learning rate\n",
    "colors = ['#55A868', '#C44E52', '#8172B2']\n",
    "labels = ['alpha=0.2', 'alpha=0.5', 'alpha=0.8']\n",
    "\n",
    "all_rewards = []\n",
    "all_steps = []\n",
    "all_avg_rewards = []\n",
    "all_avg_steps = []\n",
    "\n",
    "np.random.seed(42) # For reproducibility\n",
    "\n",
    "for idx, alpha in enumerate(alphas):\n",
    "    # Initialising Qtable before training\n",
    "    Qtable_softmax = initialize_q_table(state_space, action_space)\n",
    "    # Training with softmax\n",
    "    Qtable_softmax, softmax_rewards, softmax_steps = softmax_train(\n",
    "        n_training_episodes,\n",
    "        temp,\n",
    "        env,\n",
    "        max_steps,\n",
    "        Qtable_softmax,\n",
    "        alpha,\n",
    "        gamma\n",
    "    )\n",
    "\n",
    "    # Evaluation\n",
    "    mean_reward_softmax, std_reward_softmax = evaluate_agent(env, Qtable_softmax, max_steps)\n",
    "    print(f\"Alpahs={alpha} -> Mean reward = {mean_reward_softmax:.2f} +/- {std_reward_softmax:.2f}\")\n",
    "\n",
    "    all_rewards.append(softmax_rewards)\n",
    "    all_steps.append(softmax_steps)\n",
    "\n",
    "    def moving_average(data, window_size=50):\n",
    "        return np.convolve(data, np.ones(window_size)/window_size, mode='valid')\n",
    "\n",
    "    avg_rewards = moving_average(softmax_rewards, window_size=50)\n",
    "    avg_steps = moving_average(softmax_steps, window_size=50)\n",
    "    all_avg_rewards.append(avg_rewards)\n",
    "    all_avg_steps.append(avg_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011d9b54-cf4d-431c-b960-9c8eeb18185b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting performance\n",
    "fig, axs = plt.subplots(4, 1, figsize=(10,8), sharex=True)\n",
    "\n",
    "for idx, rewards in enumerate(all_rewards):\n",
    "    axs[0].plot(rewards, label=labels[idx], color=colors[idx])\n",
    "axs[0].set_title('Total Reward per Episode')\n",
    "axs[0].set_ylabel('Total Reward')\n",
    "axs[0].grid(True)\n",
    "axs[0].legend(loc='upper left')\n",
    "\n",
    "for idx, steps in enumerate(all_steps):\n",
    "    axs[1].plot(steps, label=labels[idx], color=colors[idx])\n",
    "axs[1].set_title('Steps per Episode')\n",
    "axs[1].set_ylabel('Steps')\n",
    "axs[1].grid(True)\n",
    "axs[1].legend(loc='upper left')\n",
    "\n",
    "for idx, avg_rewards in enumerate(all_avg_rewards):\n",
    "    axs[2].plot(range(len(avg_rewards)), avg_rewards, label=labels[idx], color=colors[idx])\n",
    "axs[2].set_title('Average Reward per Episode')\n",
    "axs[2].set_ylabel('Average Reward')\n",
    "axs[2].grid(True)\n",
    "axs[2].legend(loc='upper left')\n",
    "\n",
    "for idx, avg_steps in enumerate(all_avg_steps):\n",
    "    axs[3].plot(range(len(avg_steps)), avg_steps, label=labels[idx], color=colors[idx])\n",
    "axs[3].set_title('Average Steps per Episode')\n",
    "axs[3].set_xlabel('Episode')\n",
    "axs[3].set_ylabel('Average Steps')\n",
    "axs[3].grid(True)\n",
    "axs[3].legend(loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71cd0e8-4e49-48d6-bb9d-2fd1c4731995",
   "metadata": {},
   "source": [
    "Leaving temperature and alpha fixed while changing gamma parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c186bbb1-e6ef-449b-9904-2784cccdb913",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "n_training_episodes = 1500   # Total training episodes\n",
    "max_steps = 100   # Maximum number of steps per episode\n",
    "temperature = 2.5   # Temperature\n",
    "alpha = 0.5   # Learning rate\n",
    "\n",
    "gammas = [0.9, 0.95, 0.99]   # Discounting rate\n",
    "colors = ['#55A868', '#C44E52', '#8172B2']\n",
    "labels = ['gamma=0.9', 'gamma=0.95', 'gamma=0.99']\n",
    "\n",
    "all_rewards = []\n",
    "all_steps = []\n",
    "all_avg_rewards = []\n",
    "all_avg_steps = []\n",
    "\n",
    "np.random.seed(42) # For reproducibility\n",
    "\n",
    "for idx, temp in enumerate(temperatures):\n",
    "    # Initialising Qtable before training\n",
    "    Qtable_softmax = initialize_q_table(state_space, action_space)\n",
    "    # Training with softmax\n",
    "    Qtable_softmax, softmax_rewards, softmax_steps = softmax_train(\n",
    "        n_training_episodes,\n",
    "        temp,\n",
    "        env,\n",
    "        max_steps,\n",
    "        Qtable_softmax,\n",
    "        alpha,\n",
    "        gamma\n",
    "    )\n",
    "\n",
    "    # Evaluation\n",
    "    mean_reward_softmax, std_reward_softmax = evaluate_agent(env, Qtable_softmax, max_steps)\n",
    "    print(f\"Gammas={gamma} -> Mean reward = {mean_reward_softmax:.2f} +/- {std_reward_softmax:.2f}\")\n",
    "\n",
    "    all_rewards.append(softmax_rewards)\n",
    "    all_steps.append(softmax_steps)\n",
    "\n",
    "    def moving_average(data, window_size=50):\n",
    "        return np.convolve(data, np.ones(window_size)/window_size, mode='valid')\n",
    "\n",
    "    avg_rewards = moving_average(softmax_rewards, window_size=50)\n",
    "    avg_steps = moving_average(softmax_steps, window_size=50)\n",
    "    all_avg_rewards.append(avg_rewards)\n",
    "    all_avg_steps.append(avg_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1a4775-ca02-49c5-b25d-70481944c345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting performance\n",
    "fig, axs = plt.subplots(4, 1, figsize=(10,8), sharex=True)\n",
    "\n",
    "for idx, rewards in enumerate(all_rewards):\n",
    "    axs[0].plot(rewards, label=labels[idx], color=colors[idx])\n",
    "axs[0].set_title('Total Reward per Episode')\n",
    "axs[0].set_ylabel('Total Reward')\n",
    "axs[0].grid(True)\n",
    "axs[0].legend(loc='upper left')\n",
    "\n",
    "for idx, steps in enumerate(all_steps):\n",
    "    axs[1].plot(steps, label=labels[idx], color=colors[idx])\n",
    "axs[1].set_title('Steps per Episode')\n",
    "axs[1].set_ylabel('Steps')\n",
    "axs[1].grid(True)\n",
    "axs[1].legend(loc='upper left')\n",
    "\n",
    "for idx, avg_rewards in enumerate(all_avg_rewards):\n",
    "    axs[2].plot(range(len(avg_rewards)), avg_rewards, label=labels[idx], color=colors[idx])\n",
    "axs[2].set_title('Average Reward per Episode')\n",
    "axs[2].set_ylabel('Average Reward')\n",
    "axs[2].grid(True)\n",
    "axs[2].legend(loc='upper left')\n",
    "\n",
    "for idx, avg_steps in enumerate(all_avg_steps):\n",
    "    axs[3].plot(range(len(avg_steps)), avg_steps, label=labels[idx], color=colors[idx])\n",
    "axs[3].set_title('Average Steps per Episode')\n",
    "axs[3].set_xlabel('Episode')\n",
    "axs[3].set_ylabel('Average Steps')\n",
    "axs[3].grid(True)\n",
    "axs[3].legend(loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edee38fa-cf45-425a-8de2-c45d141ec4bd",
   "metadata": {},
   "source": [
    "Best parameter combination - which combination is the fastest to reach an average positive reward:\n",
    "\n",
    "TEMPERATURE=2.5\n",
    "\n",
    "ALPHA=0.5\n",
    "\n",
    "GAMMA=0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c52f502-d871-48f9-a26a-d6f95e9381dc",
   "metadata": {},
   "source": [
    "# ADVANCED TASKS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca29178-8fdf-458e-b861-12673fcae422",
   "metadata": {},
   "source": [
    "## DQN + Double DQN Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688316f9-ebec-4468-a58d-e3ce43f63473",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from torch import optim\n",
    "from collections import deque\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7a6fb5-e9ef-4489-8cfb-d5818599ae53",
   "metadata": {},
   "source": [
    "The great majority of the below code was inspired from lab 6 [3], and adapted for the defined environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec728b0-573b-4149-869e-eb94334abc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The below defined experience replay buffer class is used to stabilise the learning process of the double DQN algorithm\n",
    "class ExperienceReplayBuffer:\n",
    "    def __init__(self, capacity: int, random_state=None):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size: int):\n",
    "        indices = self.random_state.choice(len(self.buffer), batch_size, replace=False)\n",
    "        experiences = [self.buffer[i] for i in indices]\n",
    "        states, actions, rewards, next_states, dones = zip(*experiences)\n",
    "        return (\n",
    "        torch.tensor(states, dtype=torch.float32),\n",
    "        torch.tensor(actions, dtype=torch.int64).unsqueeze(1),\n",
    "        torch.tensor(rewards, dtype=torch.float32).unsqueeze(1),\n",
    "        torch.tensor(next_states, dtype=torch.float32),\n",
    "        torch.tensor(dones, dtype=torch.uint8).unsqueeze(1)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e2ecdb-4ee6-4cb9-bcf4-35f40ce3e7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Q-network\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size, number_hidden_units):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(state_size, number_hidden_units)\n",
    "        self.fc2 = nn.Linear(number_hidden_units, number_hidden_units)\n",
    "        self.fc3 = nn.Linear(number_hidden_units, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# Deep Q-learning Agent\n",
    "class DeepQAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_size,\n",
    "        action_size,\n",
    "        number_hidden_units,\n",
    "        optimizer_fn,\n",
    "        batch_size,\n",
    "        buffer_size,\n",
    "        epsilon_decay_schedule,\n",
    "        alpha,\n",
    "        gamma,\n",
    "        update_frequency,\n",
    "        double_dqn = False,\n",
    "        seed = 0\n",
    "    ):\n",
    "        self.random_state = np.random.RandomState(seed)\n",
    "        self.torch_generator = torch.Generator().manual_seed(seed)\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.qnetwork_local = QNetwork(state_size, action_size, number_hidden_units)\n",
    "        self.qnetwork_target = QNetwork(state_size, action_size, number_hidden_units)\n",
    "        self.optimizer = optimizer_fn(self.qnetwork_local.parameters())\n",
    "        self.batch_size = batch_size\n",
    "        self.memory = ExperienceReplayBuffer(buffer_size, self.random_state)\n",
    "        self.epsilon_decay_schedule = epsilon_decay_schedule\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.update_frequency = update_frequency\n",
    "        self.double_dqn = double_dqn\n",
    "\n",
    "        self.steps = 0\n",
    "\n",
    "    def act(self, state, epsilon=0):\n",
    "        if self.random_state.rand() > epsilon:\n",
    "            with torch.no_grad():\n",
    "                state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "                q_values = self.qnetwork_local(state)\n",
    "                return torch.argmax(q_values).item()\n",
    "\n",
    "        else:\n",
    "            return self.random_state.choice(np.arange(self.action_size))\n",
    "\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        self.steps += 1\n",
    "\n",
    "        if len(self.memory) >= self.batch_size and self.steps % self.update_frequency == 0:\n",
    "            self.learn()\n",
    "\n",
    "    def learn(self):\n",
    "        states, actions, rewards, next_states, dones = self.memory.sample(self.batch_size)\n",
    "\n",
    "        if self.double_dqn:\n",
    "            double_q_learning_update(\n",
    "                self.qnetwork_local,\n",
    "                self.qnetwork_target,\n",
    "                self.optimizer,\n",
    "                states,\n",
    "                actions,\n",
    "                rewards,\n",
    "                next_states,\n",
    "                dones,\n",
    "                self.gamma\n",
    "            )\n",
    "        else:\n",
    "            standard_q_learning_update(\n",
    "                self.qnetwork_local,\n",
    "                self.qnetwork_target,\n",
    "                self.optimizer,\n",
    "                states,\n",
    "                actions,\n",
    "                rewards,\n",
    "                next_states,\n",
    "                dones,\n",
    "                self.gamma\n",
    "            )\n",
    "\n",
    "        # Implementing soft target network update\n",
    "        for target_param, local_param in zip(self.qnetwork_target.parameters(), self.qnetwork_local.parameters()):\n",
    "            target_param.data.copy_(self.alpha * local_param.data + (1.0 - self.alpha) * target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3911b8fb-92dc-4614-845d-e8281b0d06a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining standard and double Q-learning update functions\n",
    "def standard_q_learning_update(qnetwork_local, qnetwork_target, optimizer, states, actions, rewards, next_states, dones, gamma):\n",
    "    q_targets_next = qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "    q_targets = rewards + (gamma * q_targets_next * (1 - dones.float()))\n",
    "\n",
    "    q_expected = qnetwork_local(states).gather(1, actions)\n",
    "\n",
    "    loss = F.mse_loss(q_expected, q_targets)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "def double_q_learning_update(qnetwork_local, qnetwork_target, optimizer, states, actions, rewards, next_states, dones, gamma):\n",
    "    best_actions = qnetwork_local(next_states).detach().argmax(1).unsqueeze(1)\n",
    "    q_targets_next = qnetwork_target(next_states).gather(1, best_actions)\n",
    "    q_targets = rewards + (gamma * q_targets_next * (1 - dones.float()))\n",
    "\n",
    "    q_expected = qnetwork_local(states).gather(1, actions)\n",
    "\n",
    "    loss = F.mse_loss(q_expected, q_targets)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa25a9b7-46b9-4454-9afa-1d56a83e09f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining power decay epsilon-schedule\n",
    "def power_decay_schedule(episode_number: int, decay_factor: float, minimum_epsilon: float) -> float:\n",
    "    return max(decay_factor ** episode_number, minimum_epsilon)\n",
    "\n",
    "_epsilon_decay_schedule_kwargs = {\n",
    "    \"decay_factor\": 0.99,\n",
    "    \"minimum_epsilon\": 1e-2,\n",
    "}\n",
    "epsilon_decay_schedule = lambda n: power_decay_schedule(n, **_epsilon_decay_schedule_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517abe43-3795-47ff-b072-bfbc5ad365fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer configuration\n",
    "_optimizer_kwargs = {\n",
    "    \"lr\": 1e-3,\n",
    "    \"betas\": (0.9, 0.999),\n",
    "    \"eps\": 1e-08,\n",
    "    \"weight_decay\": 0,\n",
    "    \"amsgrad\": False,\n",
    "}\n",
    "optimizer_fn = lambda parameters: optim.Adam(parameters, **_optimizer_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3991bb-d72f-4c9e-9f95-2c3a3be91d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of training function\n",
    "def train(agent, env, checkpoint_filepath, target_score=195.0, number_episodes=2000):\n",
    "    scores = []\n",
    "    scores_window = deque(maxlen=100)\n",
    "\n",
    "    for episode in range(1, number_episodes + 1):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            epsilon = agent.epsilon_decay_schedule(episode)\n",
    "            action = agent.act(state, epsilon)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "        scores.append(total_reward)\n",
    "        scores_window.append(total_reward)\n",
    "\n",
    "        print(f\"Episode {episode}\\tAverage Score: {np.mean(scores_window):.2f}\")\n",
    "\n",
    "        if np.mean(scores_window) >= target_score:\n",
    "            print(f\"\\nEnvironment solved in {episode} episodes!\")\n",
    "            torch.save(agent.qnetwork_local.state_dict(), checkpoint_filepath)\n",
    "            break\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f118c2ba-77e9-4382-80ef-81ca753abba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up environment\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# Agent configuration\n",
    "_agent_kwargs = {\n",
    "    \"state_size\": env.observation_space.shape[0],\n",
    "    \"action_size\": env.action_space.n,\n",
    "    \"number_hidden_units\": 64,\n",
    "    \"optimizer_fn\": optimizer_fn,\n",
    "    \"epsilon_decay_schedule\": epsilon_decay_schedule,\n",
    "    \"batch_size\": 64,\n",
    "    \"buffer_size\": 100000,\n",
    "    \"alpha\": 1e-3,\n",
    "    \"gamma\": 0.99,\n",
    "    \"update_frequency\": 4,\n",
    "    \"seed\": 42,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e6a8c4-f140-4e6d-a4e4-330c0fe60084",
   "metadata": {},
   "source": [
    "Training standard and double DQN agents without early stopping - but with target_score in mind (195)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5132955-34da-4617-aa65-92cfc4a76d6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Standard DQN agent\n",
    "_agent_kwargs[\"double_dqn\"] = False\n",
    "dqn_agent = DeepQAgent(**_agent_kwargs)\n",
    "dqn_scores = train(dqn_agent, env, \"dqn-checkpoint.pth\", number_episodes=2000, target_score=float(\"inf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ecee58-bd7a-4ae3-bddd-0635a003f964",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Double DQN agent\n",
    "_agent_kwargs[\"double_dqn\"] = True\n",
    "double_dqn_agent = DeepQAgent(**_agent_kwargs)\n",
    "double_dqn_scores = train(double_dqn_agent, env, \"double-dqn-checkpoint.pth\", number_episodes=2000, target_score=float(\"inf\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e579035-bb5a-4c8a-a698-21e3cf50660b",
   "metadata": {},
   "source": [
    "## Plotting Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d99fa16-fc58-4c2a-99fb-32c39d3ea283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard DQN\n",
    "dqn_scores = pd.Series(dqn_scores, name=\"DQN Scores\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 3))\n",
    "\n",
    "dqn_scores.plot(ax=ax, label=\"DQN Scores\", color='blue')\n",
    "dqn_scores.rolling(window=100).mean().rename(\"Rolling Avg\").plot(ax=ax, color='red')\n",
    "ax.legend(loc='upper left')\n",
    "ax.set_xlabel(\"Episode\")\n",
    "ax.set_ylabel(\"Score\")\n",
    "ax.set_title(\"Standard DQN Score per Episode\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694751c7-197f-417f-ba0d-d8224a5097d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double DQN\n",
    "double_dqn_scores = pd.Series(double_dqn_scores, name=\"Double DQN Scores\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 3))\n",
    "\n",
    "double_dqn_scores.plot(ax=ax, label=\"Double DQN Scores\", color='blue')\n",
    "double_dqn_scores.rolling(window=100).mean().rename(\"Rolling Avg\").plot(ax=ax, color='red')\n",
    "ax.legend(loc='upper left')\n",
    "ax.set_ylabel(\"Score\")\n",
    "ax.set_xlabel(\"Episode\")\n",
    "ax.set_title(\"Double DQN Score per Episode\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fde3e0-194d-4b0a-8da6-ba8c957d6ee9",
   "metadata": {},
   "source": [
    "## Multi-Step Learning Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22cfbff-0ba3-4e50-8f5b-84a559f11abc",
   "metadata": {},
   "source": [
    "In the following code, the double DQN multi-step learning is implemented.\n",
    "\n",
    "Most of the code is taken from above, just with some slight changes to employ the multi-step learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8f7c9d-795d-4005-b30a-77f60d70a367",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72590fd-0029-4b63-b0cf-0bd332914ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining experience replay buffer for multi-step DQN\n",
    "class ExperienceReplayBuffer:\n",
    "    def __init__(self, capacity, gamma, n_step, seed):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        self.n_step_buffer = deque(maxlen=n_step)\n",
    "        self.n_step = n_step\n",
    "        self.gamma = gamma\n",
    "        self.random = random.Random(seed)\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "\n",
    "    def _get_n_step_info(self):\n",
    "        R, discount = 0.0, 1.0\n",
    "        for (_, _, r, _, d) in self.n_step_buffer:\n",
    "            R += discount * r\n",
    "            discount *= self.gamma\n",
    "            if d:\n",
    "                break\n",
    "        state, action, _, _, _ = self.n_step_buffer[0]\n",
    "        _, _, _, next_state, done = self.n_step_buffer[-1]\n",
    "        return (state, action, R, next_state, done)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.n_step_buffer.append((state, action, reward, next_state, done))\n",
    "        if len(self.n_step_buffer) == self.n_step or done:\n",
    "            experience = self._get_n_step_info()\n",
    "            self.buffer.append(experience)\n",
    "            if done:\n",
    "                self.n_step_buffer.clear()\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        samples = self.random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*samples)\n",
    "        return (\n",
    "            torch.tensor(states, dtype=torch.float32),\n",
    "            torch.tensor(actions, dtype=torch.int64).unsqueeze(1),\n",
    "            torch.tensor(rewards, dtype=torch.float32).unsqueeze(1),\n",
    "            torch.tensor(next_states, dtype=torch.float32),\n",
    "            torch.tensor(dones, dtype=torch.uint8).unsqueeze(1),\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c932b347-ea2f-4275-b7de-a28aa36d36ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Q-network class\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263c7061-6420-4cff-aaa0-914b04181f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent using Double DQN\n",
    "class DoubleDQNAgent:\n",
    "    def __init__(self, state_size, action_size, hidden_size, gamma, alpha,\n",
    "                 batch_size, buffer_size, update_frequency, n_step, epsilon_schedule, seed):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.batch_size = batch_size\n",
    "        self.update_frequency = update_frequency\n",
    "        self.epsilon_schedule = epsilon_schedule\n",
    "        self.step_count = 0\n",
    "\n",
    "        self.q_local = QNetwork(state_size, action_size, hidden_size)\n",
    "        self.q_target = QNetwork(state_size, action_size, hidden_size)\n",
    "        self.optimizer = optim.Adam(self.q_local.parameters(), lr=1e-3)\n",
    "\n",
    "        self.buffer = ExperienceReplayBuffer(buffer_size, gamma, n_step, seed)\n",
    "        self.random = random.Random(seed)\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "    def act(self, state, episode):\n",
    "        epsilon = self.epsilon_schedule(episode)\n",
    "        if self.random.random() > epsilon:\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                return torch.argmax(self.q_local(state_tensor)).item()\n",
    "        return self.random.randint(0, self.action_size - 1)\n",
    "\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        self.buffer.add(state, action, reward, next_state, done)\n",
    "        self.step_count += 1\n",
    "\n",
    "        if len(self.buffer) >= self.batch_size and self.step_count % self.update_frequency == 0:\n",
    "            self.learn()\n",
    "\n",
    "    def learn(self):\n",
    "        states, actions, rewards, next_states, dones = self.buffer.sample(self.batch_size)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            best_actions = self.q_local(next_states).argmax(1, keepdim=True)\n",
    "            q_targets_next = self.q_target(next_states).gather(1, best_actions)\n",
    "            q_targets = rewards + (self.gamma ** self.buffer.n_step) * q_targets_next * (1 - dones.float())\n",
    "\n",
    "        q_expected = self.q_local(states).gather(1, actions)\n",
    "        loss = F.mse_loss(q_expected, q_targets)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Soft update\n",
    "        for t_param, l_param in zip(self.q_target.parameters(), self.q_local.parameters()):\n",
    "            t_param.data.copy_(self.alpha * l_param.data + (1.0 - self.alpha) * t_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5240d2-549a-4a3a-8984-cd483ebeccb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train(agent, env, num_episodes=2000):\n",
    "    scores = []\n",
    "    for episode in range(1, num_episodes + 1):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = agent.act(state, episode)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "        scores.append(total_reward)\n",
    "        print(f\"Episode {episode}\\tAverage Score: {np.mean(scores[-100:]):.2f}\")\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405271bd-9c74-4710-8b8a-f29fe143ef23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up environment\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "# Shared agent configuration\n",
    "common_agent_kwargs = {\n",
    "    \"state_size\": state_size,\n",
    "    \"action_size\": action_size,\n",
    "    \"hidden_size\": 64,\n",
    "    \"gamma\": 0.99,\n",
    "    \"alpha\": 1e-3,\n",
    "    \"batch_size\": 64,\n",
    "    \"buffer_size\": 100000,\n",
    "    \"update_frequency\": 4,\n",
    "    \"epsilon_schedule\": lambda ep: max(0.99 ** ep, 0.01),\n",
    "    \"seed\": 42,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d51b52-a1bb-472e-88f5-2273cba0e187",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Multi-step Double DQN (n=3)\n",
    "multi_step_kwargs = dict(common_agent_kwargs)\n",
    "multi_step_kwargs[\"n_step\"] = 3\n",
    "agent_multi = DoubleDQNAgent(**multi_step_kwargs)\n",
    "scores_multi = train(agent_multi, env, num_episodes=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3a6f71-636f-4abb-b7de-4b5ba3344a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting to pandas Series\n",
    "scores_multi = pd.Series(scores_multi, name=\"Double DQN (3-step)\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 3))\n",
    "\n",
    "scores_multi.plot(ax=ax, label=\"Double DQN (3-step)\", color='blue')\n",
    "scores_multi.rolling(window=100).mean().rename(\"Rolling Avg\").plot(ax=ax, color='red')\n",
    "ax.legend(loc='upper left')\n",
    "ax.set_ylabel(\"Score\")\n",
    "ax.set_xlabel(\"Episode\")\n",
    "ax.set_title(\"3-step Double DQN Score per Episode\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7946f6d0-5b6c-4211-b640-ce387b963771",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8216a211-80cd-42ca-a5cf-ea092bc5c5f4",
   "metadata": {},
   "source": [
    "[1] Johnny Code, *\"Build a Custom Gymnasium Reinforcement Learning Environment & Train w Q-Learning & Stable Baselines3,\"* YouTube, Mar. 22 2024. [Online]. Available: https://www.youtube.com/watch?v=AoGRjPt-vms&t=1251s "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7b899b-e6f2-4965-bda8-5363cabd46d6",
   "metadata": {},
   "source": [
    "[2] A. Riaz, *Lab_04_Part2_Q_Learning_with_gym,* unpublished lab tutorial, Dept. of Science\n",
    "and Technology, City St. George's, University of London, 2025."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c8665c-2968-400b-b482-3425d789aec7",
   "metadata": {},
   "source": [
    "[3] A. Riaz, *Lab_6_DoubleDQN,* unpublished lab tutorial, Dept. of Science\n",
    "and Technology, City St. George's, University of London, 2025."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac83defe-44e0-4269-92c1-fc84621c6ed9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
